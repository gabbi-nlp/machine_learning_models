{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7945c1",
   "metadata": {},
   "source": [
    "**Bag of Words Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def51286",
   "metadata": {},
   "source": [
    "Bag of Word is a Natural Language Processing technique of text modeling . Whenever we apply any algorithm in NLP, \n",
    "we are working with numbers. We cannot directly feed our text into that algorithm. Hence, Bag of Words model \n",
    "is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59926545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdceaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing(docs):\n",
    "    # creating one entire string\n",
    "    docs = ' '.join(docs)\n",
    "\n",
    "    # removing punctuation and normalizing string\n",
    "    remove_punc = str.maketrans('', '', string.punctuation)\n",
    "    cleaned_docs = docs.translate(remove_punc)\n",
    "    cleaned_docs = cleaned_docs.lower()\n",
    "    cleaned_docs = cleaned_docs.strip()\n",
    "    cleaned_docs = cleaned_docs.split()\n",
    "\n",
    "    # gets list of unique values\n",
    "    doc_vocabulary = list(set(cleaned_docs))\n",
    "\n",
    "    return doc_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82473f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_dictionary(sent):\n",
    "    sent = sent.lower()\n",
    "    sent_word_count = {}\n",
    "    \n",
    "    # counts the number of words\n",
    "    for word in sent.split():\n",
    "        if word in sent_word_count.keys():\n",
    "            sent_word_count[word] += 1\n",
    "        else:\n",
    "            sent_word_count[word] = 1\n",
    "\n",
    "    return sent_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3813f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# builds a language dictionary based on the input doc and \n",
    "# generates embeddings for each sentence in doc\n",
    "def bag_of_words_lm(docs):\n",
    "    embeddings = []\n",
    "    docs = docs.split('.')\n",
    "    doc_vocabulary = preprocessing(docs)\n",
    "    \n",
    "    # genearte embedding per sentence\n",
    "    for sentence in docs:\n",
    "        sent_embed = []\n",
    "        sent_word_count = get_sent_dictionary(sentence)\n",
    "\n",
    "        for word in doc_vocabulary:\n",
    "            # get numeric occurence of word\n",
    "            word_count_value = sent_word_count[word] if word in sent_word_count.keys() else 0\n",
    "\n",
    "            # place value at the index associated with the word in vector: sent_embed\n",
    "            sent_embed.append(word_count_value)\n",
    "        \n",
    "        # add to all sentence embeddings\n",
    "        embeddings.append(sent_embed)\n",
    "\n",
    "    print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cd397c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0, 0, 2, 0, 2, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1], [0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 4, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 2, 0, 1, 2, 1, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], [1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0, 0, 4, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    docs_example = \"Are we beginning to commend ourselves again? Or do we need, like some people, letters of recommendation to you or from you? 2 You yourselves are our letter, written on our hearts, known and read by everyone. 3 You show that you are a letter from Christ, the result of our ministry, written not with ink but with the Spirit of the living God, not on tablets of stone but on tablets of human hearts. 4 Such confidence we have through Christ before God. 5 Not that we are competent in ourselves to claim anything for ourselves, but our competence comes from God. 6 He has made us competent as ministers of a new covenantâ€”not of the letter but of the Spirit; for the letter kills, but the Spirit gives life.\"\n",
    "    \n",
    "    # call BoW language model\n",
    "    bag_of_words_lm(docs_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
